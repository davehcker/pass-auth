{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import normalize as sklearn_normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from passauth import data\n",
    "from passauth import utils\n",
    "from passauth import authenticator\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/Users/admin/Downloads/entries/data/'\n",
    "user_data = data.UserData(mypath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting user behavior\n",
    "\n",
    "When we try to draw a picture of the user behavior, we can clearly see a unique pattern to two users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot user 1\n",
    "user1 = user_data.getUserData(10)\n",
    "utils.plotUserMouseScatterMultiple(user1, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = user_data.getUserData(6)\n",
    "utils.plotUserMouseScatterMultiple(user1, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for some Machine Learning\n",
    "\n",
    "'After some failed attempts at one-shot learning using deep NNs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/Users/admin/Downloads/entries/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def rf_ova_classifier(dataX, dataY, test_size=0.3, cross_validate=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataX, dataY, test_size=test_size, random_state=30)\n",
    "    \n",
    "    if cross_validate == 1:\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "    \n",
    "    for each in range(len(np.unique(y_train))):\n",
    "        _RF = RandomForestClassifier(n_estimators=100)\n",
    "        print(f'train {list(y_train).count(each)}')\n",
    "        _RF.fit(X_train, np.where(y_train != each, -1, each))\n",
    "        res = _RF.predict(X_test)\n",
    "        \n",
    "        print(f'Y-test {list(y_test).count(each)}', end='')\n",
    "        \n",
    "        print(f'result {list(res).count(each)}', end='')\n",
    "\n",
    "      \n",
    "        \n",
    "        print(f\"Score for User {each}:\\nAccuracy: {_RF.score(X_test, np.where(y_test != each, -1, each))}\")\n",
    "\n",
    "        FAR = [(real != each) and (guess == each) for real,guess in zip(y_test, res)].count(True)\n",
    "        FRR = [(real == each) and (guess != each) for real,guess in zip(y_test, res)].count(True)\n",
    "        \n",
    "\n",
    "        print(f\"FAR: {FAR} / {list(np.where(y_test != each, -1, 0)).count(-1)}\", end=' ')\n",
    "        print(f\"FRR: {FRR} / {list(np.where(y_test == each, each, -1)).count(each)}\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def generic_ova_discriminator(dataX, dataY, test_size=0.3, cross_validate=0, method=svm.SVC, params={}, verbose=1):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataX, dataY, test_size=test_size, random_state=30)\n",
    "    \n",
    "    if cross_validate == 1:\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "    \n",
    "\n",
    "    SVM = method(**params)\n",
    "    SVM.fit(X_train, y_train)\n",
    "\n",
    "    correct = [0,0]\n",
    "    incorrect = [0,0]\n",
    "    for each in range(len(np.unique(y_train))):\n",
    "        \n",
    "        #params['class_wight'] = {each:10}\n",
    "        _SVM = method(**params)\n",
    "\n",
    "        _SVM.fit(X_train, np.where(y_train != each, -1, each))\n",
    "        res = _SVM.predict(X_test)\n",
    "        \n",
    "        FAR = [(real != each) and (guess == each) for real,guess in zip(y_test, res)].count(True)\n",
    "        FRR = [(real == each) and (guess != each) for real,guess in zip(y_test, res)].count(True)\n",
    "        \n",
    "        correct[0] += FAR #false correct\n",
    "        correct[1] += list(np.where(y_test != each, -1, 0)).count(-1)\n",
    "        \n",
    "        incorrect[0] += FRR #false rejection\n",
    "        incorrect[1] += list(y_test).count(each)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Y-test- {list(y_test).count(each)}', end='')\n",
    "            print(f' result- {list(res).count(each)}', end='')\n",
    "\n",
    "            print(f\"FAR: {FAR} / {list(np.where(y_test != each, -1, 0)).count(-1)}\", end=' ')\n",
    "            print(f\"FRR: {FRR} / {list(np.where(y_test == each, each, -1)).count(each)}\\n\")\n",
    "      \n",
    "    print(f\"FAR = {correct[0]/correct[1]*100} AND FRR = {incorrect[0]/incorrect[1]*100} \")\n",
    "        \n",
    "\n",
    "\n",
    "def k_means_discriminator(dataX, dataY, test_size=0.3, cross_validate=0):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataX, dataY, test_size=test_size, random_state=30)\n",
    "    \n",
    "    \n",
    "    if cross_validate == 1:\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "    \n",
    "\n",
    "\n",
    "    for each in range(len(np.unique(y_train))):\n",
    "\n",
    "        _k_means = KMeans(n_clusters=2)\n",
    "\n",
    "        _k_means.fit(X_train, np.where(y_train != each, -1, each))\n",
    "        res = _k_means.predict(X_test)\n",
    "        \n",
    "        print(f'Y-test {list(y_test).count(each)}', end='')\n",
    "        \n",
    "        print(f'result {list(res).count(each)}', end='')\n",
    "\n",
    "        FAR = [(real != each) and (guess == each) for real,guess in zip(y_test, res)].count(True)\n",
    "        FRR = [(real == each) and (guess != each) for real,guess in zip(y_test, res)].count(True)\n",
    "        \n",
    "\n",
    "        print(f\"FAR: {FAR} / {list(np.where(y_test != each, -1, 0)).count(-1)}\", end=' ')\n",
    "        print(f\"FRR: {FRR} / {list(np.where(y_test == each, each, -1)).count(each)}\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouse Dynamics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_params = {'min_entries':20}\n",
    "m_params = {'min_entries':20, 'padding':200}\n",
    "\n",
    "Authenticator = authenticator.Authenticator(path=mypath, k_params=k_params, m_params=m_params)\n",
    "# Raw data with (x, y, time)\n",
    "dataX, dataY = Authenticator.get_mouse_data(Authenticator.mouse_data, method=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAR = 12.749445676274945 AND FRR = 74.39024390243902 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "generic_ova_discriminator(dataX, dataY, method=svm.LinearSVC, params={'loss':'squared_hinge'},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_ova_discriminator(dataX, dataY, method=svm.SVC, params={'kernel':'linear'},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data with (x, y)\n",
    "dataX, dataY = Authenticator.get_mouse_data(Authenticator.mouse_data, method=1)\n",
    "generic_ova_discriminator(dataX, dataY, method=svm.SVC, params={'kernel':'linear'},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataX to include only n-points\n",
    "_dataX = [each[0::10] for each in dataX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_ova_discriminator(_dataX, dataY, method=svm.SVC, params={'kernel':'linear'},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_ova_discriminator(_dataX, dataY, method=svm.SVC, params={'kernel':'linear'},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_ova_discriminator(sklearn_normalize(_dataX), dataY, method=svm.SVC, params={'kernel':'linear'},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_ova_discriminator(sklearn_normalize(_dataX)*1000, dataY, method=svm.SVC, params={'kernel':'linear', 'degree':5},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataX to include only x cordinates of n-points\n",
    "_dataX = [each[1::2] for each in _dataX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataX to include only n-points\n",
    "generic_ova_discriminator(sklearn_normalize(np.cumsum(_dataX, axis=1), norm='l2', axis=0)*5000, dataY, method=svm.SVC, params={'kernel':'linear', 'degree':5},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_ova_discriminator(np.cumsum(_dataX, axis=0), dataY, method=svm.SVC, params={'kernel':'linear', 'degree':5},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only 10th points\n",
    "generic_ova_discriminator(_dataX, dataY, method=RandomForestClassifier, params={'n_estimators':20},verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataX to include only x cordinates of n-points\n",
    "_dataX = [each[0::2] for each in [each[0::1] for each in dataX]]\n",
    "generic_ova_discriminator(sklearn_normalize(_dataX, norm='l2', axis=0)*1000, dataY, method=GaussianNB, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataX, dataY, test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each in range(len(np.unique(y_train))):\n",
    "pca = PCA(10)\n",
    "each = 0\n",
    "projected = pca.fit_transform(X_train[y_train == each])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.OneClassSVM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(pca.transform(X_train) * 100, np.where(y_train != each, -1, each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_projected = pca.transform(X_test)\n",
    "#classifier.score(test_projected * 100,  np.where(y_test != each, -1, each)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.where(y_test != each, -1, each)[classifier.predict(test_projected) == each]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.where(y_test != each, -1, each)).count(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "1. Calculate the center.\n",
    "2. Calculate the directions.\n",
    "3. Bag the events into 4-8 directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX, dataY = Authenticator.get_mouse_data(Authenticator.mouse_data, method=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataX = []\n",
    "for each in dataX:\n",
    "    transformed_dataX.append((each[::2] - 500) * (each[1::2] - 250))\n",
    "transformed_dataX = np.array(transformed_dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y-test- 4 result- 51FAR: 49 / 78 FRR: 2 / 4\n",
      "\n",
      "Y-test- 8 result- 9FAR: 8 / 74 FRR: 7 / 8\n",
      "\n",
      "Y-test- 9 result- 32FAR: 26 / 73 FRR: 3 / 9\n",
      "\n",
      "Y-test- 5 result- 15FAR: 14 / 77 FRR: 4 / 5\n",
      "\n",
      "Y-test- 10 result- 10FAR: 8 / 72 FRR: 8 / 10\n",
      "\n",
      "Y-test- 7 result- 50FAR: 44 / 75 FRR: 1 / 7\n",
      "\n",
      "Y-test- 7 result- 33FAR: 29 / 75 FRR: 3 / 7\n",
      "\n",
      "Y-test- 3 result- 70FAR: 67 / 79 FRR: 0 / 3\n",
      "\n",
      "Y-test- 6 result- 45FAR: 41 / 76 FRR: 2 / 6\n",
      "\n",
      "Y-test- 8 result- 34FAR: 33 / 74 FRR: 7 / 8\n",
      "\n",
      "Y-test- 9 result- 59FAR: 51 / 73 FRR: 1 / 9\n",
      "\n",
      "Y-test- 6 result- 42FAR: 42 / 76 FRR: 6 / 6\n",
      "\n",
      "FAR = 45.67627494456763 AND FRR = 53.65853658536586 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "generic_ova_discriminator(_dataX, dataY, method=svm.SVC, params={'kernel': 'linear', 'max_iter':1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(transformed_dataX, dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(transformed_dataX, dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Quadrants:\n",
    "```\n",
    "   \\  8 |  1 /         \n",
    "     \\  |  /  2\n",
    "_______\\|/______\n",
    "   6   /|\\  3\n",
    "     /  |4 \\\n",
    "   /    |    \\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keystroke and mouse dynamics combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#anglify(Authenticator.mouse_data[0][0])\n",
    "#anglify(Authenticator.mouse_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_params = {'min_entries':15}\n",
    "m_params = {'min_entries':15, 'padding':100}\n",
    "\n",
    "Authenticator = authenticator.Authenticator(path=mypath, k_params=k_params, m_params=m_params)\n",
    "# Raw data with (x, y, time)\n",
    "dataX, dataY = Authenticator.get_mouse_data(Authenticator.mouse_data, method=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285,)"
      ]
     },
     "execution_count": 1394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(user, predicted, actual, verbose = 0):\n",
    "\n",
    "    false_positive = [(real != user) and (guess == user) for real,guess in zip(actual, predicted)].count(True)\n",
    "    false_negative = [(real == user) and (guess != user) for real,guess in zip(actual, predicted)].count(True)\n",
    "\n",
    "    total_positive = list(actual).count(user)\n",
    "    total_negative = list(actual).count(-1)\n",
    "    \n",
    "    assert total_negative != 0 \n",
    "    assert total_positive !=0\n",
    "    far = false_positive/total_negative\n",
    "    frr = false_negative/total_positive\n",
    "\n",
    "    if verbose:\n",
    "        print(f'test-positive: {total_positive}', end=' AND ')\n",
    "        print(f'test-negative: {total_negative}', end='\\n')\n",
    "\n",
    "        print(f\"FAR: {false_positive} / {total_negative} = {far*100}%\", end=' ')\n",
    "        print(f\"FRR: {false_negative} / {total_positive} = {frr*100}%\", end=' \\n')\n",
    "\n",
    "    return far, frr\n",
    "\n",
    "def n_elems(arr, n):\n",
    "    return np.array([each[::n] for each in arr])\n",
    "\n",
    "def get_angle(p0, p1=np.array([0,0]), p2=None):\n",
    "    ''' compute angle (in degrees) for p0p1p2 corner\n",
    "    Inputs:\n",
    "        p0,p1,p2 - points in the form of [x,y]\n",
    "    '''\n",
    "    if p2 is None:\n",
    "        p2 = p1 + np.array([1, 0])\n",
    "    v0 = np.array(p0) - np.array(p1)\n",
    "    v1 = np.array(p2) - np.array(p1)\n",
    "\n",
    "    angle = np.math.atan2(np.linalg.det([v0,v1]),np.dot(v0,v1))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def anglify(arr, if_cumsum=0,):\n",
    "    angle = arr[0][0:2]\n",
    "    if if_cumsum:\n",
    "        return np.cumsum([get_angle(each[0:2], p2=angle) for each in arr])\n",
    "    return np.array([get_angle(each[0:2], p2=angle) for each in arr])\n",
    "\n",
    "def anglified(arr, if_cumsum=0,):\n",
    "    return [anglify(each, if_cumsum=if_cumsum) for each in arr]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_train_test(user, \n",
    "                      X_train, X_test, y_train, y_test, \n",
    "                      cross_validate=0, \n",
    "                      method=svm.SVC, \n",
    "                      params={}, \n",
    "                      verbose=1, \n",
    "                      return_type='analysis'):\n",
    "    \n",
    "    _SVM = method(**params)\n",
    "    _SVM.fit(X_train, np.where(y_train != user, -1, user))\n",
    "    res = _SVM.predict(X_test)\n",
    "\n",
    "    test_score = score(user, res, y_test, verbose=verbose)\n",
    "\n",
    "    analysis = {'res': res, 'far': test_score[0] , 'frr': test_score[1] } \n",
    "\n",
    "    if cross_validate == 1:\n",
    "        analysis ['cross_validate_far'], analysis ['cross_validate_frr'] = score(user, _SVM.predict(X_train), y_train)\n",
    "    \n",
    "    if return_type == 'model':\n",
    "        return _SVM\n",
    "    elif return_type == 'analysis':\n",
    "        return analysis\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class User:\n",
    "    def __init__(self, user, dataX, dataY, kdataX, kdataY, test_size=0.3, raw_mouse_data=[], random_state=30):\n",
    "        self.user = user\n",
    "        self.dataX = dataX\n",
    "        self.dataY = dataY\n",
    "        self.kdataX = kdataX\n",
    "        self.kdataY = kdataY\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "        dataX, dataY, test_size=test_size, random_state=self.random_state)\n",
    "        \n",
    "        self.X_train_k, self.X_test_k, self.y_train_k, self.y_test_k = train_test_split(\n",
    "        self.kdataX, self.kdataY, test_size=test_size, random_state=self.random_state)\n",
    " \n",
    "        self.y_train = np.where(self.y_train != user, -1, 0)\n",
    "        self.y_test = np.where(self.y_test != user, -1, 0)\n",
    "        \n",
    "        self.y_train_k = np.where(self.y_train_k != user, -1, 0)\n",
    "        self.y_test_k = np.where(self.y_test_k != user, -1, 0)\n",
    "        \n",
    "        self.X_raw_mouse = np.array([x[0] for x in raw_mouse_data])\n",
    "        self.y_raw_mouse = np.array([x[1] for x in raw_mouse_data])\n",
    "        \n",
    "        self.y_raw_mouse = np.where(self.y_raw_mouse != self.user, -1, 0)\n",
    "\n",
    "        self.X_train_raw_mouse, self.X_test_raw_mouse, self.y_train_raw_mouse, self.y_test_raw_mouse = train_test_split(\n",
    "        self.X_raw_mouse, self.y_raw_mouse, test_size=test_size, random_state=self.random_state)\n",
    "\n",
    "        #set the best model configurations\n",
    "        self.best_models_keyboard = []\n",
    "        self.best_models_keyboard_o = [] #to store the classifier\n",
    "        self.best_models_mouse = []\n",
    "        self.best_models_mouse_o = []\n",
    "\n",
    "\n",
    "    def score(self, predicted, actual, verbose = 0):\n",
    "        return score(0, predicted, actual, verbose)\n",
    "\n",
    "      \n",
    "        \n",
    "    def train_test(self, \n",
    "                   cross_validate=0, \n",
    "                   method=[svm.SVC], \n",
    "                   params={}, \n",
    "                   verbose=1, \n",
    "                   transformation=(0,0), \n",
    "                   scale=0, \n",
    "                   pca=0,\n",
    "                   return_type='analysis'):\n",
    "        \n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        \n",
    "        if transformation[0] == 1: #drop every element except n-th\n",
    "            X_train = n_elems(self.X_train, transformation[1])\n",
    "            X_test = n_elems(self.X_test, transformation[1])\n",
    "\n",
    "        elif transformation[0] == 2: #calculate the angle\n",
    "            X_train = n_elems(anglified(self.X_train_raw_mouse, if_cumsum=transformation[1]), transformation[2])\n",
    "            X_test = n_elems(anglified(self.X_test_raw_mouse, if_cumsum=transformation[1]), transformation[2])\n",
    "            \n",
    "\n",
    "        else:\n",
    "            X_train = self.X_train\n",
    "            X_test = self.X_test\n",
    "            \n",
    "        if pca > 0:\n",
    "            pca = PCA(n_components=pca, svd_solver='randomized', whiten=True).fit(self.X_train)\n",
    "            X_train = pca.transform(self.X_train)\n",
    "            X_test = pca.transform(self.X_test)\n",
    "            \n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            if X_train != []:\n",
    "                X_train= scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            else:\n",
    "                X_train= scaler.fit_transform(self.X_train)\n",
    "                X_test = scaler.transform(self.X_test)\n",
    "\n",
    "\n",
    "\n",
    "        return custom_train_test(0, X_train, X_test, self.y_train, self.y_test, \n",
    "                                cross_validate=cross_validate, \n",
    "                                 method=method, \n",
    "                                 params=params, \n",
    "                                 verbose=verbose, \n",
    "                                 return_type=return_type)\n",
    "  \n",
    "    def transform_keyboard_data(self, data, transformation=(0,0), pca=0, scale=0):\n",
    "        \n",
    "        if transformation[0] == 1: #drop every element except n-th\n",
    "            data = n_elems(self.data, transformation[1])\n",
    "\n",
    "            \n",
    "        if pca > 0:\n",
    "            pca = PCA(n_components=pca, svd_solver='randomized', whiten=True).fit(self.X_train_k)\n",
    "            data = pca.transform(data)\n",
    "            \n",
    "        if scale:\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "            data = scaler.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def train_test_keyboard(self, \n",
    "                            cross_validate=0, \n",
    "                            method=[svm.SVC], \n",
    "                            params={}, \n",
    "                            verbose=1, \n",
    "                            transformation=(0,0), \n",
    "                            scale=0, \n",
    "                            pca=0,\n",
    "                            return_type='analysis',\n",
    "                            model=None):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        \n",
    "        if transformation[0] == 1: #drop every element except n-th\n",
    "            X_train = n_elems(self.X_train_k, transformation[1])\n",
    "            X_test = n_elems(self.X_test_k, transformation[1])\n",
    "\n",
    "     \n",
    "        else:\n",
    "            X_train = self.X_train_k\n",
    "            X_test = self.X_test_k\n",
    "            \n",
    "        if pca > 0:\n",
    "            pca = PCA(n_components=pca, svd_solver='randomized', whiten=True).fit(self.X_train_k)\n",
    "            X_train = pca.transform(self.X_train_k)\n",
    "            X_test = pca.transform(self.X_test_k)\n",
    "            \n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            if X_train != []:\n",
    "                X_train= scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            else:\n",
    "                X_train= scaler.fit_transform(self.X_train_k)\n",
    "                X_test = scaler.transform(self.X_test_k)\n",
    "\n",
    "        return custom_train_test(0, X_train, X_test, self.y_train, self.y_test, \n",
    "                          cross_validate=cross_validate, method=method, params=params, verbose=verbose, return_type=return_type)\n",
    "  \n",
    "\n",
    "\n",
    "    def pipeline_train_test(self, parameters, verbose=0, keyboard=0):\n",
    "        res = []\n",
    "        for index, each in enumerate(parameters):\n",
    "            if verbose != 0:\n",
    "                print(each)\n",
    "            if keyboard != 0:\n",
    "                result = dict(self.train_test_keyboard(**each, verbose=verbose))\n",
    "            else:\n",
    "                result = dict(self.train_test(**each, verbose=verbose))\n",
    "            result['config'] = each\n",
    "            result['index'] = index\n",
    "            res.append(result)\n",
    "        return res\n",
    "\n",
    "    def set_training_models(self, kb, mouse, n=5):\n",
    "        self.best_models_keyboard = [_['config'] for _ in kb][:5]\n",
    "        self.best_models_mouse = [_['config'] for _ in mouse][:5]\n",
    "        \n",
    "    def train_training_models(self):\n",
    "        assert len(self.best_models_keyboard) != 0 and len(self.best_models_mouse) != 0, 'first find the best models'\n",
    "        \n",
    "        self.best_models_keyboard_o = [self.train_test_keyboard(**params, return_type='model') for params in self.best_models_keyboard]\n",
    "        self.best_models_mouse_o = [self.train_test(**params, return_type='model') for params in self.best_models_keyboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    # SVM kernel\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'degree':4, 'max_iter':5000}},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}, 'transformation': (1, 3)},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}, 'transformation': (2, 1, 3)},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}, 'transformation': (2, 1, 3), 'scale':1},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}, 'pca':5, 'scale': 1},\n",
    "    \n",
    "    # Gaussian\n",
    "    {'method':GaussianNB},\n",
    "    {'method':GaussianNB, 'transformation': (1, 3), 'pca':5},\n",
    "    {'method':GaussianNB, 'transformation': (1, 5)},\n",
    "    {'method':GaussianNB, 'transformation': (2, 1, 3)},\n",
    "    {'method':GaussianNB, 'transformation': (2, 0, 5)},\n",
    "\n",
    "    # Bernoulli\n",
    "    {'method':BernoulliNB},\n",
    "    {'method':BernoulliNB, 'transformation': (1, 3)},\n",
    "    {'method':BernoulliNB, 'transformation': (1, 5)},\n",
    "    {'method':BernoulliNB, 'transformation': (2, 1, 3)},\n",
    "    {'method':BernoulliNB, 'transformation': (2, 0, 5)},\n",
    "    \n",
    "    # Random Forest\n",
    "    {'method':RandomForestClassifier},\n",
    "    {'method':RandomForestClassifier, 'params': {'max_features': 'sqrt'}},\n",
    "    {'method':RandomForestClassifier, 'params': {'n_estimators': 10}},\n",
    "    {'method':RandomForestClassifier, 'transformation': (1, 10)},\n",
    "    {'method':RandomForestClassifier, 'transformation': (2, 1, 3)},\n",
    "    {'method':RandomForestClassifier, 'transformation': (2, 1, 10)},\n",
    "]\n",
    "\n",
    "\n",
    "keyboard_pipeline = [\n",
    "    # SVM kernel\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'degree':4, 'max_iter':5000}},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}, 'transformation': (1, 3)},\n",
    "    {'method':svm.SVC, 'params': {'kernel': 'linear', 'max_iter':5000}, 'pca':5, 'scale': 1},\n",
    "    \n",
    "    # Gaussian\n",
    "    {'method':GaussianNB},\n",
    "    {'method':GaussianNB, 'transformation': (1, 3), 'pca':5},\n",
    "    {'method':GaussianNB, 'transformation': (1, 5)},\n",
    "\n",
    "\n",
    "    # Bernoulli\n",
    "    {'method':BernoulliNB},\n",
    "    {'method':BernoulliNB, 'transformation': (1, 3)},\n",
    "    {'method':BernoulliNB, 'transformation': (1, 5)},\n",
    "   \n",
    "    # Random Forest\n",
    "    {'method':RandomForestClassifier},\n",
    "    {'method':RandomForestClassifier, 'params': {'max_features': 'sqrt'}},\n",
    "    {'method':RandomForestClassifier, 'params': {'n_estimators': 10}},\n",
    "    {'method':RandomForestClassifier, 'transformation': (1, 10)},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = set(each[1] for each in Authenticator.mouse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdataX = []\n",
    "kdataY = []\n",
    "for each in Authenticator.keystroke:\n",
    "    kdataX.append(utils.bin(each[0]+each[1]))\n",
    "    kdataY.append(each[2])\n",
    "kdataY = np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grand_result_mouse = {}\n",
    "grand_result_keyboard = {}\n",
    "\n",
    "user_objects = {}\n",
    "for each in users:\n",
    "    user_objects[each] = User(each, dataX, dataY, kdataX, kdataY, raw_mouse_data=Authenticator.mouse_data)\n",
    "    grand_result_mouse[each] = user_objects[each].pipeline_train_test(pipeline)\n",
    "    grand_result_keyboard[each] = user_objects[each].pipeline_train_test(keyboard_pipeline, keyboard=1)\n",
    "    \n",
    "    user_objects[each].set_training_models(optimize_models(grand_result_keyboard[each], 'balanced'), \n",
    "                                           optimize_models(grand_result_mouse[each], 'balanced'),\n",
    "                                           n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(user):\n",
    "    mouse = np.sum([_['res'] for _ in optimize_models(grand_result_mouse[user], 'balanced')[:5]], axis=0)\n",
    "    keyboard = np.sum([_['res'] for _ in optimize_models(grand_result_keyboard[user], 'balanced')[:5]], axis=0)\n",
    "    \n",
    "    combined = mouse + keyboard\n",
    "    \n",
    "    mouse = np.where(mouse > -3, 0, -1)\n",
    "    keyboard = np.where(keyboard > -3, 0, -1)\n",
    "    combined = np.where(combined > -6, 0, -1)\n",
    "    \n",
    "    return (user_objects[user].score(mouse, user_objects[user].y_test),\n",
    "            user_objects[user].score(keyboard, user_objects[user].y_test_k),\n",
    "            user_objects[user].score(combined, user_objects[user].y_test)\n",
    "           )\n",
    "    #print('Mouse only score: ', user_objects[user].score(mouse, user_objects[user].y_test))\n",
    "    #print('Keyboard only score: ', user_objects[user].score(keyboard, user_objects[user].y_test_k))\n",
    "    #print('Combined score: ', user_objects[user].score(combined, user_objects[user].y_test))\n",
    "    \n",
    "mouse_score = [0, 0]\n",
    "keyboard_score = [0, 0]\n",
    "combined_score = [0, 0]\n",
    "\n",
    "for each in users:\n",
    "    result = final_test(each)\n",
    "    mouse_score[0] += result[0][0]\n",
    "    mouse_score[1] += result[0][1]\n",
    "    keyboard_score[0] += result[1][0]\n",
    "    keyboard_score[1] += result[1][1]\n",
    "    combined_score[0] += result[2][0]\n",
    "    combined_score[1] += result[2][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse:  0.051467717533847476 0.6648351648351648\n",
      "Keyboard:  0.15129839931695588 0.38986568986568987\n",
      "Combined:  0.04352127019883322 0.5078144078144079\n"
     ]
    }
   ],
   "source": [
    "print('Mouse: ', mouse_score[0]/len(users), mouse_score[1]/len(users))\n",
    "print('Keyboard: ', keyboard_score[0]/len(users), keyboard_score[1]/len(users))\n",
    "print('Combined: ', combined_score[0]/len(users), combined_score[1]/len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_what(what):\n",
    "    if what == 'frr':\n",
    "        return 'far'\n",
    "    elif what == 'far':\n",
    "        return 'frr'\n",
    "    \n",
    "def optimize_models(models, what):\n",
    "    if what == 'balanced':\n",
    "        return list(filter(lambda x: x['far'] !=0 and x[other_what('far')] != 1, sorted(models, key=lambda x: (x['far'], x['frr']))))\n",
    "    return list(filter(lambda x: x[what] !=0 and x[other_what(what)] != 1, sorted(models, key=lambda x: x[what])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_user(models, what='far', metric='log'):\n",
    "    best_models = optimize_models(models, what)\n",
    "    res = np.zeros(len(models[0]['res']))\n",
    "    for each in best_models:\n",
    "        res = res + scale_prediction(each, metric) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.35714285714285715)\n",
      "(0.02702702702702703, 0.42857142857142855)\n",
      "(0.021621621621621623, 0.5714285714285714)\n",
      "(0.0, 0.5714285714285714)\n",
      "(0.17894736842105263, 0.2222222222222222)\n",
      "(0.015789473684210527, 0.3333333333333333)\n",
      "(0.021739130434782608, 0.5333333333333333)\n",
      "(0.010869565217391304, 0.5333333333333333)\n",
      "(0.016042780748663103, 0.5833333333333334)\n",
      "(0.03208556149732621, 0.5833333333333334)\n",
      "(0.04864864864864865, 0.2857142857142857)\n",
      "(0.0, 0.35714285714285715)\n",
      "(0.2722222222222222, 0.10526315789473684)\n",
      "(0.011111111111111112, 0.47368421052631576)\n",
      "(0.022222222222222223, 0.2631578947368421)\n",
      "(0.0, 0.2631578947368421)\n",
      "(0.05434782608695652, 0.06666666666666667)\n",
      "(0.010869565217391304, 0.13333333333333333)\n",
      "(0.25136612021857924, 0.25)\n",
      "(0.00546448087431694, 0.6875)\n",
      "(0.03910614525139665, 0.3)\n",
      "(0.0111731843575419, 0.3)\n",
      "(0.02717391304347826, 0.6)\n",
      "(0.010869565217391304, 0.6)\n",
      "(0.016483516483516484, 0.35294117647058826)\n",
      "(0.0, 0.4117647058823529)\n"
     ]
    }
   ],
   "source": [
    "# RUNNING ON THE TRAINING SET\n",
    "for user in users:\n",
    "    user0 = User(user, dataX, dataY, kdataX, kdataY, raw_mouse_data=Authenticator.mouse_data)\n",
    "    print(user0.score(np.where(optimize_user(grand_result_keyboard[user], 'balanced', metric='linear') > -3, 0, -1), user0.y_train_k))\n",
    "    print(user0.score(np.where(optimize_user(grand_result_mouse[user], 'balanced', metric='linear') > -3, 0, -1), user0.y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing best models on randomly split data\n",
      "[0.12658227848101267, 0.8571428571428571] [0.012658227848101266, 1.0] [0.012658227848101266, 1.0]\n",
      "[0.13924050632911394, 1.8571428571428572] [0.08860759493670886, 1.8571428571428572] [0.012658227848101266, 2.0]\n",
      "[0.1512886991001983, 2.857142857142857] [0.5825834985511666, 1.8571428571428572] [0.060850998932438616, 3.0]\n",
      "[0.4424279396065274, 3.7142857142857144] [0.645874637791673, 2.857142857142857] [0.09882568247674242, 4.0]\n",
      "[0.45541495259354037, 4.714285714285714] [0.7757447676618028, 3.857142857142857] [0.13778672143778137, 5.0]\n",
      "[0.5060478639859455, 5.714285714285714] [0.8643523625985117, 4.857142857142858] [0.15044494928588265, 6.0]\n",
      "[0.5560478639859455, 6.714285714285714] [1.6768523625985117, 4.857142857142858] [0.22544494928588266, 7.0]\n",
      "[0.669971914618857, 7.714285714285714] [1.7148270461428154, 5.857142857142858] [0.22544494928588266, 8.0]\n",
      "[0.732471914618857, 8.714285714285715] [1.7898270461428154, 6.857142857142858] [0.28794494928588266, 9.0]\n",
      "[0.8114192830399096, 9.614285714285716] [2.263511256669131, 7.357142857142858] [0.3668923177069353, 9.9]\n",
      "[0.8364192830399096, 10.614285714285716] [2.326011256669131, 8.357142857142858] [0.4043923177069353, 10.9]\n",
      "[0.9228390361263294, 11.614285714285716] [2.5111964418543162, 9.357142857142858] [0.41673799671928097, 11.9]\n",
      "Summary : \n",
      " MOUSE:  0.07690325301052746 0.967857142857143 \n",
      "KEYBOARD:  0.20926637015452634 ,  0.7797619047619048 \n",
      "COMBINED:  0.03472816639327341 ,  0.7797619047619048\n"
     ]
    }
   ],
   "source": [
    "# RANDOMIZED TEST SET\n",
    "import random\n",
    "print('Testing best models on randomly split data')\n",
    "\n",
    "mouse_score = [0, 0]\n",
    "keyboard_score = [0, 0]\n",
    "combined_score = [0, 0]\n",
    "\n",
    "    \n",
    "for user in users:\n",
    "    _user = User(user, dataX, dataY, kdataX, kdataY, raw_mouse_data=Authenticator.mouse_data, test_size=0.2, random_state=random.randint(0,100))\n",
    "    keyboard = _user.pipeline_train_test(user_objects[user].best_models_keyboard, keyboard=1)\n",
    "    mouse = _user.pipeline_train_test(user_objects[user].best_models_mouse)\n",
    "\n",
    "\n",
    "    mouse = np.sum([_['res'] for _ in mouse], axis=0)\n",
    "\n",
    "    keyboard = np.sum([_['res'] for _ in keyboard], axis=0)\n",
    "\n",
    "    combined = mouse + keyboard\n",
    "\n",
    "    mouse = user_objects[user].score(np.where(mouse > -3, 0, -1), user_objects[user].y_test)\n",
    "    keyboard = user_objects[user].score(np.where(keyboard > -3, 0, -1), user_objects[user].y_test_k)\n",
    "    combined = user_objects[user].score(np.where(combined > -5, 0, -1), user_objects[user].y_test)\n",
    "    \n",
    "    mouse_score[0] += mouse[0]\n",
    "    mouse_score[1] += mouse[1]\n",
    "    keyboard_score[0] += keyboard[0]\n",
    "    keyboard_score[1] += keyboard[1]\n",
    "    combined_score[0] += combined[0]\n",
    "    combined_score[1] += combined[1]\n",
    "\n",
    "    print (mouse_score, keyboard_score, combined_score, end='\\n')\n",
    "print('Summary : \\n', 'MOUSE: ', mouse_score[0]/len(users), mouse_score[1]/len(users) ,\n",
    "      '\\nKEYBOARD: ', keyboard_score[0]/len(users), ', ', keyboard_score[1]/len(users), \n",
    "      '\\nCOMBINED: ', combined_score[0]/len(users), ', ', keyboard_score[1]/len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'res': array([-1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,\n",
       "         -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       "  'far': 0.05263157894736842,\n",
       "  'frr': 0.6666666666666666,\n",
       "  'config': {'method': sklearn.svm.classes.SVC,\n",
       "   'params': {'kernel': 'linear', 'max_iter': 5000}},\n",
       "  'index': 0},\n",
       " {'res': array([-1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,\n",
       "         -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       "  'far': 0.05263157894736842,\n",
       "  'frr': 0.6666666666666666,\n",
       "  'config': {'method': sklearn.svm.classes.SVC,\n",
       "   'params': {'kernel': 'linear', 'max_iter': 5000}},\n",
       "  'index': 1},\n",
       " {'res': array([-1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,\n",
       "         -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       "  'far': 0.05263157894736842,\n",
       "  'frr': 0.6666666666666666,\n",
       "  'config': {'method': sklearn.svm.classes.SVC,\n",
       "   'params': {'kernel': 'linear', 'degree': 4, 'max_iter': 5000}},\n",
       "  'index': 2},\n",
       " {'res': array([-1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1,\n",
       "          0, -1, -1, -1, -1,  0, -1, -1, -1,  0, -1, -1,  0, -1, -1, -1, -1,\n",
       "         -1, -1, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1,  0, -1, -1, -1, -1,\n",
       "          0, -1, -1, -1, -1, -1,  0, -1,  0, -1,  0, -1, -1,  0]),\n",
       "  'far': 0.18421052631578946,\n",
       "  'frr': 0.3333333333333333,\n",
       "  'config': {'method': sklearn.naive_bayes.GaussianNB,\n",
       "   'transformation': (1, 3),\n",
       "   'pca': 5},\n",
       "  'index': 3},\n",
       " {'res': array([-1,  0,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0, -1, -1,  0,\n",
       "         -1,  0, -1,  0, -1,  0, -1,  0,  0,  0,  0,  0, -1,  0,  0,  0, -1,\n",
       "          0,  0, -1,  0,  0,  0, -1, -1,  0, -1,  0,  0,  0,  0, -1, -1,  0,\n",
       "         -1, -1, -1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0, -1,\n",
       "          0, -1, -1,  0,  0, -1, -1, -1,  0,  0,  0, -1, -1,  0]),\n",
       "  'far': 0.631578947368421,\n",
       "  'frr': 0.3333333333333333,\n",
       "  'config': {'method': sklearn.naive_bayes.GaussianNB},\n",
       "  'index': 4}]"
      ]
     },
     "execution_count": 1354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_models(user_objects[11].pipeline_train_test(user_objects[11].best_models_keyboard, keyboard=1), 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_prediction(model, metric='linear'):\n",
    "    if metric == 'linear':\n",
    "        return model['res']\n",
    "    else:\n",
    "        negative_penalty = (1-model['far'])**2 \n",
    "        positive_penalty = (1-model['frr'])**2\n",
    "        return np.where(model['res'] == -1, -1*negative_penalty, positive_penalty)\n",
    "    \n",
    "#sorted(sorted(sorted(grand_result[0],key=lambda x: x['far']), key=lambda x: x['frr']), key=lambda x: x['frr']+x['far'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 1406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scale_predictions(models):\n",
    "    res = np.array([0 for _ in models[0]['res']])\n",
    "    for each in models:\n",
    "        res = res + scale_prediction(each)\n",
    "    return res\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'passauth.data' from '../passauth/data.py'>"
      ]
     },
     "execution_count": 1511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors Affecting:\n",
    "1. No. of minimum training size.\n",
    "\n",
    "\n",
    "```15 - total_users=12\n",
    "MOUSE:  0.055247286051179345 0.6953373015873016 \n",
    "KEYBOARD:  0.05533615060873145 ,  0.673015873015873 \n",
    "COMBINED:  0.024196920923874644 ,  0.673015873015873```\n",
    "\n",
    "```15 total_users=12; test_size=0.4\n",
    " MOUSE:  0.07690325301052746 0.967857142857143 \n",
    "KEYBOARD:  0.20926637015452634 ,  0.7797619047619048 \n",
    "COMBINED:  0.03472816639327341 ,  0.7797619047619048```\n",
    "\n",
    "```12- total_users=13; train_test_size=0.3; test_size=0.2\n",
    " MOUSE:  0.0476162196107935 0.6256410256410256 \n",
    "KEYBOARD:  0.13938477484510384 ,  0.5341880341880342 \n",
    "COMBINED:  0.0270226053027813 ,  0.5341880341880342```\n",
    "\n",
    "\n",
    "2. Padding the data\n",
    "3. No. of modeles\n",
    "4. Weighing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_time = 0\n",
    "#for each in range(len(Authenticator.mouse_data)):\n",
    "#    total_time = total_time + max(np.array(Authenticator.mouse_data[each][0])[:,2])-Authenticator.mouse_data[each][0][0][2]\n",
    "timings = [max(np.array(Authenticator.mouse_data[each][0])[:,2])-Authenticator.mouse_data[each][0][0][2] \n",
    "     for each in range(len(Authenticator.mouse_data))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per session  7155.59578754615\n",
      "Shortest session  319.52499999897555\n",
      "Longest session  72661.1499999999\n",
      "Median session time  5472.030000062659\n",
      "Std. of session time  8766.772315240503\n"
     ]
    }
   ],
   "source": [
    "print('Average time per session ', sum(timings)/len(timings))\n",
    "print('Shortest session ', min(timings))\n",
    "print('Longest session ', max(timings))\n",
    "print('Median session time ', np.median(timings))\n",
    "print('Std. of session time ', np.std(timings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
